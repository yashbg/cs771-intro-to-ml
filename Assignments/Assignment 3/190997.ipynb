{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS771A Assignment 3\n",
    "\\- Yash Gupta (190997)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bc771a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['variance', 'skewness', 'curtosis', 'entropy', 'class']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# names of the columns of the dataset\n",
    "cols = ['variance', 'skewness', 'curtosis', 'entropy', 'class']\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1f16f5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.66610</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.16740</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.63830</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.52280</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.45520</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>0.40614</td>\n",
       "      <td>1.34920</td>\n",
       "      <td>-1.4501</td>\n",
       "      <td>-0.55949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>-1.38870</td>\n",
       "      <td>-4.87730</td>\n",
       "      <td>6.4774</td>\n",
       "      <td>0.34179</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>-3.75030</td>\n",
       "      <td>-13.45860</td>\n",
       "      <td>17.5932</td>\n",
       "      <td>-2.77710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>-3.56370</td>\n",
       "      <td>-8.38270</td>\n",
       "      <td>12.3930</td>\n",
       "      <td>-1.28230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>-2.54190</td>\n",
       "      <td>-0.65804</td>\n",
       "      <td>2.6842</td>\n",
       "      <td>1.19520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1372 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      variance  skewness  curtosis  entropy  class\n",
       "0      3.62160   8.66610   -2.8073 -0.44699      0\n",
       "1      4.54590   8.16740   -2.4586 -1.46210      0\n",
       "2      3.86600  -2.63830    1.9242  0.10645      0\n",
       "3      3.45660   9.52280   -4.0112 -3.59440      0\n",
       "4      0.32924  -4.45520    4.5718 -0.98880      0\n",
       "...        ...       ...       ...      ...    ...\n",
       "1367   0.40614   1.34920   -1.4501 -0.55949      1\n",
       "1368  -1.38870  -4.87730    6.4774  0.34179      1\n",
       "1369  -3.75030 -13.45860   17.5932 -2.77710      1\n",
       "1370  -3.56370  -8.38270   12.3930 -1.28230      1\n",
       "1371  -2.54190  -0.65804    2.6842  1.19520      1\n",
       "\n",
       "[1372 rows x 5 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the data\n",
    "data = pd.read_csv('data_banknote_authentication.txt', names=cols)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. (a)\n",
    "Code up the perceptron algorithm described on slide 7 of Lecture 15 using the same notation as in the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the perceptron algorithm\n",
    "def perceptron_algo(X, y, thresh=10000, eta=1, max_iter=10000):\n",
    "    w = np.array([0.5, 0.5, 0.5, 0.5])\n",
    "    t = 0\n",
    "    t_mistake = 0\n",
    "    while t - t_mistake < thresh and t < max_iter:\n",
    "        idx = np.random.choice(np.arange(len(X)))\n",
    "        X_n = X.loc[idx, :]\n",
    "        y_n = y[idx]\n",
    "        if y_n * np.dot(w, X_n) < 0:\n",
    "            w += eta * y_n * X_n\n",
    "            t_mistake = t\n",
    "        t += 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. (b)\n",
    "Write functions to make predictions using the algorithm for the banknotes dataset. Preprocess the dataset to handle missing and anomalous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes predictions using the given w\n",
    "def predict(w, X):\n",
    "    y = np.zeros(len(X))\n",
    "    for i in range(len(X)):\n",
    "        y[i] = 1 if np.dot(w, X.loc[i, :]) >= 0 else -1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the accuracy using the predicted and actual values\n",
    "def accuracy(y_pred, y_actual):\n",
    "    return (y_pred == y_actual).sum() / len(y_actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variance    0\n",
       "skewness    0\n",
       "curtosis    0\n",
       "entropy     0\n",
       "class       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for missing data\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variance    float64\n",
       "skewness    float64\n",
       "curtosis    float64\n",
       "entropy     float64\n",
       "class         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the data types of the columns\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.66610</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.16740</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.63830</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.52280</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.45520</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>0.40614</td>\n",
       "      <td>1.34920</td>\n",
       "      <td>-1.4501</td>\n",
       "      <td>-0.55949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>-1.38870</td>\n",
       "      <td>-4.87730</td>\n",
       "      <td>6.4774</td>\n",
       "      <td>0.34179</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>-3.75030</td>\n",
       "      <td>-13.45860</td>\n",
       "      <td>17.5932</td>\n",
       "      <td>-2.77710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>-3.56370</td>\n",
       "      <td>-8.38270</td>\n",
       "      <td>12.3930</td>\n",
       "      <td>-1.28230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>-2.54190</td>\n",
       "      <td>-0.65804</td>\n",
       "      <td>2.6842</td>\n",
       "      <td>1.19520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1372 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      variance  skewness  curtosis  entropy  class\n",
       "0      3.62160   8.66610   -2.8073 -0.44699     -1\n",
       "1      4.54590   8.16740   -2.4586 -1.46210     -1\n",
       "2      3.86600  -2.63830    1.9242  0.10645     -1\n",
       "3      3.45660   9.52280   -4.0112 -3.59440     -1\n",
       "4      0.32924  -4.45520    4.5718 -0.98880     -1\n",
       "...        ...       ...       ...      ...    ...\n",
       "1367   0.40614   1.34920   -1.4501 -0.55949      1\n",
       "1368  -1.38870  -4.87730    6.4774  0.34179      1\n",
       "1369  -3.75030 -13.45860   17.5932 -2.77710      1\n",
       "1370  -3.56370  -8.38270   12.3930 -1.28230      1\n",
       "1371  -2.54190  -0.65804    2.6842  1.19520      1\n",
       "\n",
       "[1372 rows x 5 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing the negative class label from 0 to -1\n",
    "data['class'].replace(0, -1, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.66610</td>\n",
       "      <td>-2.80730</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.16740</td>\n",
       "      <td>-2.45860</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.63830</td>\n",
       "      <td>1.92420</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.52280</td>\n",
       "      <td>-4.01120</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.45520</td>\n",
       "      <td>4.57180</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>-1.16670</td>\n",
       "      <td>-1.42370</td>\n",
       "      <td>2.92410</td>\n",
       "      <td>0.66119</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>-2.41000</td>\n",
       "      <td>3.74330</td>\n",
       "      <td>-0.40215</td>\n",
       "      <td>-1.29530</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>0.40614</td>\n",
       "      <td>1.34920</td>\n",
       "      <td>-1.45010</td>\n",
       "      <td>-0.55949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>-1.38870</td>\n",
       "      <td>-4.87730</td>\n",
       "      <td>6.47740</td>\n",
       "      <td>0.34179</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>-2.54190</td>\n",
       "      <td>-0.65804</td>\n",
       "      <td>2.68420</td>\n",
       "      <td>1.19520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1280 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      variance  skewness  curtosis  entropy  class\n",
       "0      3.62160   8.66610  -2.80730 -0.44699     -1\n",
       "1      4.54590   8.16740  -2.45860 -1.46210     -1\n",
       "2      3.86600  -2.63830   1.92420  0.10645     -1\n",
       "3      3.45660   9.52280  -4.01120 -3.59440     -1\n",
       "4      0.32924  -4.45520   4.57180 -0.98880     -1\n",
       "...        ...       ...       ...      ...    ...\n",
       "1363  -1.16670  -1.42370   2.92410  0.66119      1\n",
       "1366  -2.41000   3.74330  -0.40215 -1.29530      1\n",
       "1367   0.40614   1.34920  -1.45010 -0.55949      1\n",
       "1368  -1.38870  -4.87730   6.47740  0.34179      1\n",
       "1371  -2.54190  -0.65804   2.68420  1.19520      1\n",
       "\n",
       "[1280 rows x 5 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove outliers using the interquartile range method\n",
    "q25, q75 = np.percentile(data.drop('class', axis=1), 25, axis=0), np.percentile(data.drop('class', axis=1), 75, axis=0)\n",
    "iqr = q75 - q25\n",
    "cut_off = iqr * 1.5\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "data = data.loc[[(((data.drop('class', axis=1).loc[i] >= lower) & (data.drop('class', axis=1).loc[i] <= upper)).sum() == 4) for i in range(len(data))], :]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yashbg/.local/lib/python3.9/site-packages/pandas/core/indexing.py:1676: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.078409</td>\n",
       "      <td>1.202941</td>\n",
       "      <td>-1.037066</td>\n",
       "      <td>0.299985</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.423167</td>\n",
       "      <td>1.109087</td>\n",
       "      <td>-0.939557</td>\n",
       "      <td>-0.229004</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.169569</td>\n",
       "      <td>-0.924506</td>\n",
       "      <td>0.286027</td>\n",
       "      <td>0.588391</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.016865</td>\n",
       "      <td>1.364168</td>\n",
       "      <td>-1.373718</td>\n",
       "      <td>-1.340177</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.149622</td>\n",
       "      <td>-1.266440</td>\n",
       "      <td>1.026388</td>\n",
       "      <td>0.017640</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>-0.707598</td>\n",
       "      <td>-0.695923</td>\n",
       "      <td>0.565634</td>\n",
       "      <td>0.877474</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>-1.171342</td>\n",
       "      <td>0.276488</td>\n",
       "      <td>-0.364502</td>\n",
       "      <td>-0.142082</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>-0.120939</td>\n",
       "      <td>-0.174073</td>\n",
       "      <td>-0.657545</td>\n",
       "      <td>0.241360</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>-0.790403</td>\n",
       "      <td>-1.345878</td>\n",
       "      <td>1.559261</td>\n",
       "      <td>0.711030</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>-1.220540</td>\n",
       "      <td>-0.551828</td>\n",
       "      <td>0.498549</td>\n",
       "      <td>1.155755</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1280 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      variance  skewness  curtosis   entropy  class\n",
       "0     1.078409  1.202941 -1.037066  0.299985     -1\n",
       "1     1.423167  1.109087 -0.939557 -0.229004     -1\n",
       "2     1.169569 -0.924506  0.286027  0.588391     -1\n",
       "3     1.016865  1.364168 -1.373718 -1.340177     -1\n",
       "4    -0.149622 -1.266440  1.026388  0.017640     -1\n",
       "...        ...       ...       ...       ...    ...\n",
       "1363 -0.707598 -0.695923  0.565634  0.877474      1\n",
       "1366 -1.171342  0.276488 -0.364502 -0.142082      1\n",
       "1367 -0.120939 -0.174073 -0.657545  0.241360      1\n",
       "1368 -0.790403 -1.345878  1.559261  0.711030      1\n",
       "1371 -1.220540 -0.551828  0.498549  1.155755      1\n",
       "\n",
       "[1280 rows x 5 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardizing the features except the target variable\n",
    "for col in cols[:-1]:\n",
    "    data.loc[:, col] = (data.loc[:, col] - data.loc[:, col].mean()) / data.loc[:, col].std()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('class', axis=1)\n",
    "y = data['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. (c)\n",
    "Train the algorithm on the dataset using cross-validation and report cross-validated test set error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the training, validation and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1.604480</td>\n",
       "      <td>1.115712</td>\n",
       "      <td>-0.986256</td>\n",
       "      <td>-0.005967</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>-1.249671</td>\n",
       "      <td>-1.718168</td>\n",
       "      <td>1.486416</td>\n",
       "      <td>0.208342</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>-0.052427</td>\n",
       "      <td>0.969671</td>\n",
       "      <td>0.093386</td>\n",
       "      <td>-1.009321</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1.272701</td>\n",
       "      <td>-1.120400</td>\n",
       "      <td>0.818424</td>\n",
       "      <td>1.382179</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>-0.577237</td>\n",
       "      <td>0.208342</td>\n",
       "      <td>-1.277860</td>\n",
       "      <td>-1.268054</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>-1.284844</td>\n",
       "      <td>1.727519</td>\n",
       "      <td>0.337759</td>\n",
       "      <td>-1.532207</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>-0.932514</td>\n",
       "      <td>0.218072</td>\n",
       "      <td>-0.591635</td>\n",
       "      <td>-0.706762</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>-0.009503</td>\n",
       "      <td>-0.395667</td>\n",
       "      <td>-0.751447</td>\n",
       "      <td>0.721140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0.339508</td>\n",
       "      <td>0.239884</td>\n",
       "      <td>0.138435</td>\n",
       "      <td>0.343107</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>-1.013976</td>\n",
       "      <td>-0.239895</td>\n",
       "      <td>-0.331916</td>\n",
       "      <td>0.167909</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1280 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      variance  skewness  curtosis   entropy  class\n",
       "137   1.604480  1.115712 -0.986256 -0.005967     -1\n",
       "1213 -1.249671 -1.718168  1.486416  0.208342      1\n",
       "405  -0.052427  0.969671  0.093386 -1.009321     -1\n",
       "255   1.272701 -1.120400  0.818424  1.382179     -1\n",
       "1019 -0.577237  0.208342 -1.277860 -1.268054      1\n",
       "...        ...       ...       ...       ...    ...\n",
       "732  -1.284844  1.727519  0.337759 -1.532207     -1\n",
       "945  -0.932514  0.218072 -0.591635 -0.706762      1\n",
       "1161 -0.009503 -0.395667 -0.751447  0.721140      1\n",
       "242   0.339508  0.239884  0.138435  0.343107     -1\n",
       "1117 -1.013976 -0.239895 -0.331916  0.167909      1\n",
       "\n",
       "[1280 rows x 5 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffling the rows of the dataframe\n",
    "data_shuffle = data.sample(frac=1, random_state=1)\n",
    "data_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing train-test split with 20% of the data as the test data\n",
    "train_size = int(0.8 * len(data))\n",
    "data_train = data_shuffle[:train_size].reset_index(drop=True)\n",
    "data_test = data_shuffle[train_size:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change cross-val method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the dataset for cross-validation\n",
    "def data_split(data):\n",
    "    data_shuffle = data.sample(frac=1)\n",
    "    data_shuffle\n",
    "    train_size = int(0.8 * len(data))\n",
    "    data_train = data_shuffle[:train_size].reset_index(drop=True)\n",
    "    data_val = data_shuffle[train_size:].reset_index(drop=True)\n",
    "    return data_train, data_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's perform the cross-validation and tune the hyperparameter `thresh` which is the number of no-mistake iterations after which we can say that the algorithm has converged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross-validated accuracy is 0.9717073170731707 using a threshold of 150\n"
     ]
    }
   ],
   "source": [
    "accuracy_dict = {}\n",
    "for k in range(5):\n",
    "    data_train_crossval, data_val = data_split(data_train)\n",
    "    X_train = data_train_crossval.drop('class', axis=1)\n",
    "    y_train = data_train_crossval['class']\n",
    "    X_val = data_val.drop('class', axis=1)\n",
    "    y_val = data_val['class']\n",
    "    for thresh in range(50, 500, 50):\n",
    "        if k == 0:\n",
    "            accuracy_dict[thresh] = 0\n",
    "        w = perceptron_algo(X_train, y_train, thresh, 1, 10000)\n",
    "        y_pred = predict(w, X_val)\n",
    "        acc = accuracy(y_pred, y_val)\n",
    "        accuracy_dict[thresh] += acc\n",
    "best_acc = 0\n",
    "best_thresh = 0\n",
    "for thresh, acc in accuracy_dict.items():\n",
    "    acc /= 5\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_thresh = thresh\n",
    "print('The cross-validated accuracy is', best_acc, 'using a threshold of', best_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. (d)\n",
    "Ensure you use a held out validation set and report F1 score on the held out set for your best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the perceptron algorithm on the test set using the optimal threshold we got from cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test set accuracy is 0.984375 using a threshold of 150\n"
     ]
    }
   ],
   "source": [
    "X_train = data_train.drop('class', axis=1)\n",
    "y_train = data_train['class']\n",
    "X_test = data_test.drop('class', axis=1)\n",
    "y_test = data_test['class']\n",
    "w = perceptron_algo(X_train, y_train, best_thresh, 1, 10000)\n",
    "y_pred = predict(w, X_test)\n",
    "acc = accuracy(y_pred, y_test)\n",
    "print('The test set accuracy is', acc, 'using a threshold of', best_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on the test set is 0.9814814814814815 using a threshold of 150\n"
     ]
    }
   ],
   "source": [
    "tp = ((y_pred == y_test) & (y_test == 1)).sum()\n",
    "fn = ((y_pred != y_test) & (y_test == 1)).sum()\n",
    "fp = ((y_pred != y_test) & (y_test == -1)).sum()\n",
    "tn = ((y_pred == y_test) & (y_test == -1)).sum()\n",
    "p = tp / (tp + fp)\n",
    "r = tp / (tp + fn)\n",
    "f1 = 2 * p * r / (p + r)\n",
    "print('F1 score on the test set is', f1, 'using a threshold of', best_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. MCMC Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. (a)\n",
    "Write a function to calculate the Bayesian posterior probability given 50 new data samples drawn from a normal distribution with mean 10 and SD 5, assuming a normal prior with mean 25 and s.d. 5. Plot the pdfs of the prior, the likelihood and the posterior distributions. Explain how you derive the likelihood from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Distribution:\n",
    "$$ p(x | \\mu, \\sigma^2) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left( - \\frac{1}{2} \\left( \\frac{x - \\mu}{\\sigma} \\right)^2 \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal likelihood:\n",
    "$$ p(\\textbf{y} | \\mu, \\sigma^2) = \\prod_{n = 1}^N p(y_n | \\mu, \\sigma) = \\frac{1}{\\sigma^N (2 \\pi)^{\\frac{N}{2}}} \\exp \\left(- \\frac{1}{2 \\sigma^2} \\sum_{n = 1}^N \\left( y_n - \\mu \\right)^2 \\right) $$\n",
    "$$ = \\frac{1}{\\sigma^N (2 \\pi)^{\\frac{N}{2}}} \\exp \\left( - \\frac{1}{2 \\sigma^2} \\left( \\sum_n y_n^2 - 2 \\mu \\sum_n y_n + N \\mu^2 \\right) \\right) $$\n",
    "$$ = \\frac{1}{\\sigma^N (2 \\pi)^{\\frac{N}{2}}} \\exp \\left( - \\frac{1}{2 \\sigma^2} \\sum_n y_n^2 \\right) \\exp \\left( - \\frac{N}{2 \\sigma^2} \\left(- 2 \\mu \\overline{y} + \\mu^2 \\right) \\right) $$\n",
    "$$ = \\frac{1}{\\sigma^N (2 \\pi)^{\\frac{N}{2}}} \\exp \\left( - \\frac{1}{2 \\sigma^2} \\left( \\sum_n y_n^2 - N \\overline{y}^2 \\right) \\right) \\exp \\left( - \\frac{N}{2 \\sigma^2} (\\mu - \\overline{y})^2 \\right) $$\n",
    "$$ \\propto \\exp \\left( - \\frac{N}{2 \\sigma^2} (\\mu - \\overline{y})^2 \\right) $$\n",
    "$$ \\propto N \\left( \\overline{y}, \\frac{\\sigma^2}{N} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal prior:\n",
    "$$ p(\\mu | \\mu_0, \\sigma_0^2) = \\frac{1}{\\sigma_0 \\sqrt{2 \\pi}} \\exp \\left( - \\frac{1}{2} \\left( \\frac{\\mu - \\mu_0}{\\sigma_0} \\right)^2 \\right) $$\n",
    "<!-- $$ p(\\mu | \\mu_0, \\sigma_0^2) = p(\\mu | 25, 25) = \\frac{1}{5 \\sqrt{2 \\pi}} \\exp \\left( - \\frac{1}{2} \\left( \\frac{\\mu - 25}{5} \\right)^2 \\right) $$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posterior:\n",
    "$$ p(\\mu | \\textbf{y}) = \\frac{p(\\mu) p(\\textbf{y} | \\mu)}{p(\\textbf{y})} $$\n",
    "$$ \\propto p(\\mu) p(\\textbf{y} | \\mu) $$\n",
    "<!-- $$ \\propto p(\\mu) p(\\overline{y} | \\mu) $$ -->\n",
    "<!-- $$ \\propto p(\\overline{y}) p(\\mu | \\overline{y}) $$\n",
    "$$ \\propto p(\\mu | \\overline{y}) $$ -->\n",
    "$$ \\propto \\exp \\left( - \\frac{1}{2} \\left( \\frac{\\mu - \\mu_0}{\\sigma_0} \\right)^2 \\right) \\exp \\left( - \\frac{N}{2 \\sigma^2} (\\mu - \\overline{y})^2 \\right) $$\n",
    "$$ \\propto \\exp \\left( - \\frac{1}{2} \\left( \\left( \\frac{\\mu - \\mu_0}{\\sigma_0} \\right)^2 + \\left( \\frac{N}{\\sigma^2} (\\mu - \\overline{y})^2 \\right) \\right) \\right) $$\n",
    "$$ \\propto \\exp \\left( - \\frac{1}{2} \\left( \\frac{\\mu^2}{\\sigma_0^2} - \\frac{2 \\mu \\mu_0}{\\sigma_0^2} + \\frac{\\mu_0^2}{\\sigma_0^2} + \\frac{N \\overline{y}^2}{\\sigma^2} - \\frac{2 N \\overline{y} \\mu}{\\sigma^2} + \\frac{N \\mu^2}{\\sigma^2} \\right) \\right) $$\n",
    "$$ \\propto \\exp \\left( - \\frac{1}{2} \\left( \\left( \\frac{1}{\\sigma_0^2} + \\frac{N}{\\sigma^2} \\right) \\mu^2 - 2 \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\frac{N \\overline{y}}{\\sigma^2} \\right) \\mu + \\left( \\frac{\\mu_0^2}{\\sigma_0^2} + \\frac{N \\overline{y}^2}{\\sigma^2} \\right) \\right) \\right) $$\n",
    "$$ \\propto \\exp \\left( - \\frac{1}{2} \\left( \\frac{1}{\\sigma_0^2} + \\frac{N}{\\sigma^2} \\right) \\left( \\mu^2 - 2 \\left( \\frac{\\frac{\\mu_0}{\\sigma_0^2} + \\frac{N \\overline{y}}{\\sigma^2}}{\\frac{1}{\\sigma_0^2} + \\frac{N}{\\sigma^2}} \\right) \\mu + \\frac{\\frac{\\mu_0^2}{\\sigma_0^2} + \\frac{N \\overline{y}^2}{\\sigma^2}}{\\frac{1}{\\sigma_0^2} + \\frac{N}{\\sigma^2}} \\right) \\right) $$\n",
    "$$ \\propto \\exp \\left( - \\frac{1}{2} \\left( \\frac{1}{\\sigma_0^2} + \\frac{N}{\\sigma^2} \\right) \\left( \\mu - \\frac{\\frac{\\mu_0}{\\sigma_0^2} + \\frac{N \\overline{y}}{\\sigma^2}}{\\frac{1}{\\sigma_0^2} + \\frac{N}{\\sigma^2}} \\right)^2 \\right) $$\n",
    "$$ \\propto \\exp \\left( - \\frac{1}{2} \\frac{\\left( \\mu - \\frac{\\frac{\\mu_0}{\\sigma_0^2} + \\frac{N \\overline{y}}{\\sigma^2}}{\\frac{1}{\\sigma_0^2} + \\frac{N}{\\sigma^2}} \\right)^2}{\\left( \\frac{1}{\\sigma_0^2} + \\frac{N}{\\sigma^2} \\right)^{-1}} \\right) $$\n",
    "\n",
    "<!-- where\n",
    "$$ p(\\textbf{y}) = \\int p(\\mu) p(\\textbf{y} | \\mu) d \\theta $$\n",
    "\n",
    "Hence, \n",
    "$$ p(\\mu | \\textbf{y}) = \\frac{\\frac{1}{5 \\sqrt{2 \\pi}} \\exp \\left(- \\frac{1}{2} \\left( \\frac{\\mu - 25}{5} \\right)^2 \\right) \\frac{1}{\\sigma^N (2 \\pi)^{\\frac{N}{2}}} \\exp \\left( - \\frac{1}{2 \\sigma^2} \\sum_{n = 1}^N \\left( y_n - \\mu \\right)^2 \\right)}{p(\\textbf{y})} $$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, Posterior:\n",
    "$$ p(\\mu | \\textbf{y}) \\sim N(\\mu_1, \\sigma_1^2) = \\frac{1}{\\sigma_1 \\sqrt{2 \\pi}} \\exp \\left( - \\frac{1}{2} \\left( \\frac{\\mu - \\mu_1}{\\sigma_1} \\right)^2 \\right) $$\n",
    "where\n",
    "$$ \\mu_1 = \\frac{\\frac{\\mu_0}{\\sigma_0^2} + \\frac{N \\overline{y}}{\\sigma^2}}{\\frac{1}{\\sigma_0^2} + \\frac{N}{\\sigma^2}} $$\n",
    "$$ \\sigma_1^2 = \\left( \\frac{1}{\\sigma_0^2} + \\frac{N}{\\sigma^2} \\right)^{-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the functions to calculate the normal, likelihood and posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(x, mu=0, sigma=1):\n",
    "    exp = -(((x - mu) / sigma) ** 2) / 2\n",
    "    p = np.exp(exp) / (sigma * np.sqrt(2 * np.pi))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw above that the normal likelihood is proportional to $ N(\\overline{y}, \\frac{\\sigma^2}{N}) $. Hence, we will plot the likelihood as $ N(\\overline{y}, \\frac{\\sigma^2}{N}) $ because $ \\prod_{n = 1}^N p(y_n | \\mu, \\sigma) $ will give a very small likelihood which would have to be scaled up anyway to be able to be compared with the prior and the posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def likelihood(mu, y, sigma=1):\n",
    "#     l = 1\n",
    "#     for y_n in y:\n",
    "#         l *= normal(y_n, mu, sigma)\n",
    "#     return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(mu, y, sigma=1):\n",
    "    n = len(y)\n",
    "    y_bar = 0\n",
    "    for y_n in y:\n",
    "        y_bar += y_n\n",
    "    y_bar /= n\n",
    "\n",
    "    sigma_l = sigma / np.sqrt(n)\n",
    "    l = normal(mu, y_bar, sigma_l)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior(mu, y, mu0=0, sigma0=1, sigma=1):\n",
    "    n = len(y)\n",
    "    y_bar = 0\n",
    "    for y_n in y:\n",
    "        y_bar += y_n\n",
    "    y_bar /= n\n",
    "\n",
    "    num_mu1 = (mu0 / (sigma0 ** 2)) + (n * y_bar / (sigma ** 2))\n",
    "    den_mu1 = (1 / (sigma0 ** 2)) + (n / (sigma ** 2))\n",
    "    mu1 = num_mu1 / den_mu1\n",
    "\n",
    "    sigma1_sq_inv = (1 / (sigma0 ** 2)) + (n / (sigma ** 2))\n",
    "    sigma1_sq = 1 / sigma1_sq_inv\n",
    "    sigma1 = np.sqrt(sigma1_sq)\n",
    "\n",
    "    return normal(mu, mu1, sigma1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the prior, likelihood and posterior on the same graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.linspace(5, 45, 1000)\n",
    "# y = np.random.normal(10, 5, 50)\n",
    "# prior = normal(x, 25, 5)\n",
    "# l = likelihood(x, y, 5)\n",
    "# post = posterior(x, y, 25, 5, 5)\n",
    "# plt.plot(x, prior / prior.max(), label='prior')\n",
    "# plt.plot(x, l / l.max(), label='likelihood') #\n",
    "# plt.plot(x, post / post.max(), label='posterior') #\n",
    "# plt.xlabel('x')\n",
    "# plt.ylabel('p')\n",
    "# plt.title('Bayesian Posterior Probability')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(5, 45, 1000)\n",
    "y = np.random.normal(10, 5, 50)\n",
    "prior = normal(x, 25, 5)\n",
    "l = likelihood(x, y, 5)\n",
    "post = posterior(x, y, 25, 5, 5)\n",
    "plt.plot(x, prior, label='prior')\n",
    "plt.plot(x, l, label='likelihood') #\n",
    "plt.plot(x, post, label='posterior') #\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p')\n",
    "plt.title('Bayesian Posterior Probability')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. (b)\n",
    "Implement the Metropolis algorithm from the lecture slides to estimate the posterior distribution given the same prior and data and show that it converges to the analytic posterior by plotting a histogram of samples from the distribution alongside the analytic posterior distribution. Assume whatever SD (width) you want for the proposal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metropolis-Hastings algorithm which runs for `num_iter` iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hastings(prior, likelihood, y, mu0=0, sigma0=1, sigma=1, num_iter=1000, proposal_width=1):\n",
    "    hist = [0.0]\n",
    "    mu_last = 0.0\n",
    "    for i in range(num_iter):\n",
    "        mu_curr = np.random.normal(mu_last, proposal_width)\n",
    "        r = (prior(mu_curr, mu0, sigma0) * likelihood(mu_curr, y, sigma)) / (prior(mu_last, mu0, sigma0) * likelihood(mu_last, y, sigma))\n",
    "        a = np.random.uniform(0, 1)\n",
    "        if r <= a:\n",
    "            mu_curr = mu_last\n",
    "        hist.append(mu_curr)\n",
    "        mu_last = mu_curr\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the histogram we get from the Metropolis-Hastings algorithm and the posterior we calculated before to see if the Metropolis-Hastings algorithm is able to estimate the posterior distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = metropolis_hastings(normal, likelihood, y, 25, 5, 5)\n",
    "x = np.linspace(0, 15, 500)\n",
    "y_hist, x_hist, _ = plt.hist(hist, 50, label='histogram')\n",
    "post = posterior(x, y, 25, 5, 5)\n",
    "plt.plot(x, post / post.max() * y_hist.max(), label='posterior')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p')\n",
    "plt.title('Metropolis-Hastings')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the histogram is able to estimate the posterior distribution fairly well when we took the proposal width to be 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. (c)\n",
    "How does the speed of convergence of the sampling depend on the proposal width? Is there an optimal proposal width that would work best? Demonstrate the consequences of using sub-optimal proposal width and terminating sampling too soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will introduce `thresh` and `max_iter` in the Metropolis-Hastings algorithm to see for which proposal width the algorithm converges the fastest (takes the least number of iterations to converge). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hastings2(prior, likelihood, y, mu0=0, sigma0=1, sigma=1, max_iter=10000, thresh=100, proposal_width=1):\n",
    "    hist = [0.0]\n",
    "    mu_last = 0.0\n",
    "    t = 0\n",
    "    t_change = 0\n",
    "    while t - t_change < thresh and t < max_iter:\n",
    "        mu_curr = np.random.normal(mu_last, proposal_width)\n",
    "        r = (prior(mu_curr, mu0, sigma0) * likelihood(mu_curr, y, sigma)) / (prior(mu_last, mu0, sigma0) * likelihood(mu_last, y, sigma))\n",
    "        a = np.random.uniform(0, 1)\n",
    "        if r <= a:\n",
    "            mu_curr = mu_last\n",
    "        hist.append(mu_curr)\n",
    "        if mu_curr != mu_last:\n",
    "            t_change = t\n",
    "        mu_last = mu_curr\n",
    "        t += 1\n",
    "    return hist, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find out the optimal width using the new Metropolis-Hastings algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal width is 15 which takes 112 iterations to converge.\n"
     ]
    }
   ],
   "source": [
    "min_t = float('inf')\n",
    "optimal_width = 0\n",
    "for width in range(5, 50, 5):\n",
    "    hist, t = metropolis_hastings2(normal, likelihood, y, 25, 5, 5, 10000, 100, width)\n",
    "    if t < min_t:\n",
    "        min_t = t\n",
    "        optimal_width = width\n",
    "print('Optimal width is', optimal_width, 'which takes', min_t, 'iterations to converge.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the histogram using the optimal width to see if the algorithm is able to estimate the posterior distribution properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, t = metropolis_hastings2(normal, likelihood, y, 25, 5, 5, 10000, 100, optimal_width)\n",
    "x = np.linspace(0, 15, 500)\n",
    "y_hist, x_hist, _ = plt.hist(hist, 50, range=(0, 20), label='histogram')\n",
    "post = posterior(x, y, 25, 5, 5)\n",
    "plt.plot(x, post / post.max() * y_hist.max(), label='posterior')\n",
    "plt.xlim([5, 15])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p')\n",
    "plt.title('Metropolis-Hastings')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, using a proposal width of 35, the algorithm takes the least number of iterations to converge. But the histogram is not able to estimate the posterior distribution properly, as there are too little samples to create the histogram. \n",
    "\n",
    "On the other hand, using a sub-optimal proposal width, although takes more time to converge, but gives a lot of samples and hence creates a much better histogram which is able to estimate the posterior distribution properly as we say before. \n",
    "\n",
    "So, there is a tradeoff between the speed of convergence and the accuracy of the histogram in estimation of the posterior distribution. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
