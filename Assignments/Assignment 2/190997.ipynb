{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CS771A Assignment 2\n",
    "\\- Yash Gupta (190997)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. Gradient Descent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import time as t"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# defining the gradient descent function\n",
    "def gradient_descent(gradient, init_, learn_rate, n_iter=50, tol=1e-06):\n",
    "    x = init_\n",
    "    for _ in range(n_iter):\n",
    "        delta = -learn_rate * gradient(x)\n",
    "        if np.all(np.abs(delta) <= tol):\n",
    "            break\n",
    "        x += delta\n",
    "    return np.round(x, 3) # Use np.round() instead of round() to round a numpy array for the multivariate case"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (a)\n",
    "Use this function to find minima for (i) $x^2 + 3x + 4$ and (ii) $x^4 â€“ 3x^2 + 2x$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we'll have to find the gradient of the expressions in (i) and (ii). As these are expressions in one variable, their gradients are simply there derivatives wrt. x. They can be given as follows:  \n",
    "(i) $2x + 3$  \n",
    "(ii) $4x^3 - 6x + 2$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have the gradients, we can find the minima as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (i)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "min_x1 = gradient_descent(gradient=lambda x: 2 * x + 3, init_=0.0, learn_rate=0.05, n_iter=500)\n",
    "print('Point of minima:', min_x1)\n",
    "min_y1 = min_x1 ** 2 + 3 * min_x1 + 4\n",
    "print('Minima:', min_y1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Point of minima: -1.5\n",
      "Minima: 1.75\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (ii)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "min_x2 = gradient_descent(gradient=lambda x: 4 * (x ** 3) - 6 * x + 2, init_=0.0, learn_rate=0.05, n_iter=500)\n",
    "print('Point of minima:', min_x2)\n",
    "min_y2 = min_x2 ** 4 - 3 * (min_x2 ** 2) + 2 * min_x2\n",
    "print('Minima:', min_y2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Point of minima: -1.366\n",
      "Minima: -4.848076206064\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (b)\n",
    "Write a gradient function to calculate gradients for a linear regression $y = ax + b$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The loss function ($L_2$ loss) for a linear regression $y = ax + b$, say $L(a, b)$, will be: \n",
    "$$ L(a, b) = \\sum_{n = 1}^N (y_n - a x_n - b)^2 $$\n",
    "where N is the number of data points.\n",
    "\n",
    "The gradient for this loss function will be:\n",
    "$$ \\left[ \\frac{\\partial L}{\\partial a}, \\frac{\\partial L}{\\partial b} \\right] = \\left[ -2 \\sum_{n = 1}^N x_n (y_n - a x_n - b), -2 \\sum_{n = 1}^N (y_n - a x_n - b) \\right] $$\n",
    "\n",
    "Hence, the gradient function can be written as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def gradient_lr(params):\n",
    "    grad = np.array([0.0, 0.0])\n",
    "    for Xn, yn in zip(X, y): # X is the features and y is the labels both of which will be global variables\n",
    "        grad[0] += -2 * Xn * (yn - params[0] * Xn - params[1])\n",
    "        grad[1] += -2 * (yn - params[0] * Xn - params[1])\n",
    "    grad /= len(X) # so that grad is not very large and doesn't depend on the number of data points\n",
    "    return grad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (c)\n",
    "Generate artificial data for this regression according to the following protocol and use gradient descent to find the optimal parameters relating X with y."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "np.random.seed(0)\n",
    "X = 2.5 * np.random.randn(10000) + 1.5 # array of 10000 values with mean = 1.5, stddev = 2.5\n",
    "res = 1.5 * np.random.randn(10000) # generate 10000 residual terms\n",
    "y = 2 + 0.3 * X + res # actual values of y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running gradient descent on this data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "params_init = np.array([0.0, 0.0])\n",
    "tic = t.process_time()\n",
    "params = gradient_descent(gradient_lr, init_=params_init, learn_rate=0.05, n_iter=1000)\n",
    "toc = t.process_time()\n",
    "time_vanilla_gd = toc - tic\n",
    "a = params[0]\n",
    "b = params[1]\n",
    "print('a =', a)\n",
    "print('b =', b)\n",
    "print('Time taken:', time_vanilla_gd, 'seconds')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a = 0.295\n",
      "b = 2.023\n",
      "Time taken: 3.325558632 seconds\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (d)\n",
    "Implement minibatch stochastic gradient descent using the code base you have developed so far."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can define the minibatch stochastic gradient descent function by creating a minibatch in every iteration of the gradient descent and passing the minibatch to the gradient function as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# defining the minibatch stochastic gradient descent function\n",
    "def minibatch_sgd(gradient, init_, learn_rate, n_iter=50, tol=1e-06, batch_size=125):\n",
    "    x = init_\n",
    "    for _ in range(n_iter):\n",
    "        idx = np.random.choice(np.arange(len(X)), batch_size, replace=False) # indexes of the minibatch\n",
    "        X_batch = X[idx]\n",
    "        y_batch = y[idx]\n",
    "        delta = -learn_rate * gradient(x, X_batch, y_batch)\n",
    "        if np.all(np.abs(delta) <= tol):\n",
    "            break\n",
    "        x += delta\n",
    "    return np.round(x, 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will also need to define a gradient function which takes the minibatches as inputs as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def gradient_lr_sgd(params, X_batch, y_batch):\n",
    "    grad = np.array([0.0, 0.0])\n",
    "    for Xn, yn in zip(X_batch, y_batch):\n",
    "        grad[0] += -2 * Xn * (yn - params[0] * Xn - params[1])\n",
    "        grad[1] += -2 * (yn - params[0] * Xn - params[1])\n",
    "    grad /= len(X_batch) # so that grad is not very large and doesn't depend on the size of the batch\n",
    "    return grad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running minibatch stochastic gradient descent on the data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "params_sgd_init = np.array([0.0, 0.0])\n",
    "tic = t.process_time()\n",
    "params_sgd = minibatch_sgd(gradient_lr_sgd, init_=params_sgd_init, learn_rate=0.05, n_iter=1000)\n",
    "toc = t.process_time()\n",
    "time_minibatch_sgd = toc - tic\n",
    "a = params[0]\n",
    "b = params[1]\n",
    "print('a =', a)\n",
    "print('b =', b)\n",
    "print('Time taken:', time_minibatch_sgd, 'seconds')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a = 0.295\n",
      "b = 2.023\n",
      "Time taken: 0.4765957890000001 seconds\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (e)\n",
    "Does SGD do better or worse in terms of time performance on our data? Is there an optimal minibatch size that works best? Quantify and interpret your findings."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also find the time taken by SGD using a single training example per iteration. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "params_sgd_init_ = np.array([0.0, 0.0])\n",
    "tic = t.process_time()\n",
    "params_sgd = minibatch_sgd(gradient_lr_sgd, init_=params_sgd_init_, learn_rate=0.05, n_iter=1000, batch_size=1) # As SGD is just minibatch SGD with the minibatch size equal to 1\n",
    "toc = t.process_time()\n",
    "time_sgd = toc - tic\n",
    "a = params[0]\n",
    "b = params[1]\n",
    "print('a =', a)\n",
    "print('b =', b)\n",
    "print('Time taken:', time_sgd, 'seconds')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a = 0.295\n",
      "b = 2.023\n",
      "Time taken: 0.2541064430000004 seconds\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at the times taken by vanilla gradient descent and minibatch SGD."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print('Time taken by vanilla gradient descent:', time_vanilla_gd)\n",
    "print('Time taken by SGD:', time_sgd)\n",
    "print('Time taken by minibatch SGD:', time_minibatch_sgd)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time taken by vanilla gradient descent: 3.325558632\n",
      "Time taken by SGD: 0.2541064430000004\n",
      "Time taken by minibatch SGD: 0.4765957890000001\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, SGD and minibatch SGD do much better in terms of time performance than vanilla gradient descent. \n",
    "\n",
    "Vanilla gradient descent uses all the data in each iteration whereas SGD and minibatch SGD use only a part of the data in each iteration, which reduces the time of each iteration and hence take less time then vanilla gradient descent. \n",
    "\n",
    "We found that the times taken by the gradient descents were as follows:  \n",
    "t<sub>SGD</sub> < t<sub>minibatch SGD</sub> < t<sub>vanilla gradient descent</sub>\n",
    "\n",
    "So, SGD appears to be working the fastest. However, it should be noted that as SGD uses only a single data point in each iteration, the updates would be unstable and it may take more number of iterations for SGD to converge, or, may have a higher loss as compared to minibatch SGD. But for same number of iterations, SGD would take the least time as it uses only a single data point in each iteration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's find the optimal minibatch size that works best. We will consider both the time taken as well as the loss. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, let's define a function to find the $L_2$ loss:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def l2_loss(params):\n",
    "    loss = 0.0\n",
    "    for i in range(len(X)):\n",
    "        loss += (y[i] - params[0] * X[i] - params[1]) ** 2\n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's iterate over some minibatch sizes and see which works the best. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "params_sgd_init = np.array([0.0, 0.0])\n",
    "min_time = float('inf')\n",
    "min_loss = float('inf')\n",
    "min_time_batch_size = 0\n",
    "min_loss_batch_size = 0\n",
    "for batch_size in range(125, 1001, 125):\n",
    "    # print('Current batch size:', batch_size)\n",
    "    tic = t.process_time()\n",
    "    params_sgd = minibatch_sgd(gradient_lr_sgd, init_=params_sgd_init, learn_rate=0.1, n_iter=500, batch_size=batch_size)\n",
    "    toc = t.process_time()\n",
    "    time_sgd = toc - tic\n",
    "    loss_sgd = l2_loss(params_sgd)\n",
    "    if time_sgd < min_time:\n",
    "        min_time = time_sgd\n",
    "        min_time_batch_size = batch_size\n",
    "    if loss_sgd < min_loss:\n",
    "        min_loss = loss_sgd\n",
    "        min_loss_batch_size = batch_size\n",
    "    print(f'For a batch size of {batch_size}: time taken = {round(time_sgd, 3)} seconds and L2 loss = {round(loss_sgd, 3)}')\n",
    "print()\n",
    "print('The least time taking minibatch size is', min_time_batch_size, 'and it takes', min_time, 'seconds')\n",
    "print('The least loss minibatch size is', min_loss_batch_size, 'and it gives a loss of', min_loss)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For a batch size of 125: time taken = 0.281 seconds and L2 loss = 22625.428\n",
      "For a batch size of 250: time taken = 0.332 seconds and L2 loss = 22321.531\n",
      "For a batch size of 375: time taken = 0.532 seconds and L2 loss = 22214.284\n",
      "For a batch size of 500: time taken = 0.657 seconds and L2 loss = 22374.676\n",
      "For a batch size of 625: time taken = 0.771 seconds and L2 loss = 22228.737\n",
      "For a batch size of 750: time taken = 0.906 seconds and L2 loss = 22485.522\n",
      "For a batch size of 875: time taken = 1.04 seconds and L2 loss = 22256.803\n",
      "For a batch size of 1000: time taken = 1.185 seconds and L2 loss = 22189.813\n",
      "\n",
      "The least time taking minibatch size is 125 and it takes 0.2811508429999998 seconds\n",
      "The least loss minibatch size is 1000 and it gives a loss of 22189.813166395394\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the time taken increases as the batch size increases but the minimum loss can be seen with a batch size of 375. \n",
    "\n",
    "As the time taken for a batch size of 375 is also pretty low, we can say that this is the optimal minibatch size that works best. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q2. Bayesian network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q2. (i)\n",
    "Calculate the probability that someone has both cold and a fever"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let $C$ represent cold and $F$ represent fever. So, we need to find $P(C \\cap F)$. \n",
    "$$ P(C \\cap F) = P(F | C) P(C) = 0.307 \\times 0.02 = 0.00614 $$\n",
    "Hence, the probability that someone has both cold and a fever is 0.00614"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q2. (ii)\n",
    "Calculate the probability that someone who has a cough has a cold"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let $X$ represent cough, $C$ represent cold, $L$ represent lung disease and $S$ represent smokes. So, we need to find $P(C | X)$. \n",
    "$$ P(C | X) = \\frac{P(X | C) P(C)}{P(X)} $$\n",
    "Now, $$ P(X | C) = P \\bigl( X | (C \\cap L) \\bigr) P(L | C) + P \\bigl( X | (C \\cap \\overline{L}) \\bigr) P(\\overline{L} | C) $$\n",
    "Since C and L are independent, $P(L | C) = P(L)$ and $P(\\overline{L} | C) = P(\\overline{L})$.\n",
    "So, $$ P(X | C) = P \\bigl( X | (C \\cap L) \\bigr) P(L) + P \\bigl( X | (C \\cap \\overline{L}) \\bigr) P(\\overline{L}) $$\n",
    "Now, $$ P(L) = P(L | S) P(S) + P(L | \\overline{S}) P(\\overline{S}) = 0.1009 \\times 0.2 + 0.001 \\times 0.8 = 0.02098 $$\n",
    "and $$ P(\\overline{L}) = 1 - P(L) = 1 - 0.02098 = 0.97902 $$\n",
    "Hence, $$ P(X | C) = 0.7525 \\times 0.02098 + 0.505 \\times 0.97902 = 0.51019255 $$\n",
    "Now, $$ P(X) = P \\bigl(X | (L \\cap C) \\bigr) P(L \\cap C) + P \\bigl(X | (L \\cap \\overline{C}) \\bigr) P(L \\cap \\overline{C}) + P \\bigl(X | (\\overline{L} \\cap C) \\bigr) P(\\overline{L} \\cap C) + P \\bigl(X | (\\overline{L} \\cap \\overline{C}) \\bigr) P(\\overline{L} \\cap \\overline{C}) $$\n",
    "Again, since L and C are independent, $P(L \\cap C) = P(L) P(C)$,  $P(L \\cap \\overline{C}) = P(L) P(\\overline{C})$, $P(\\overline{L} \\cap C) = P(\\overline{L}) P(C)$ and $P(\\overline{L} \\cap \\overline{C}) = P(\\overline{L}) P(\\overline{C})$  \n",
    "Hence, $$ P(X) = P \\bigl(X | (L \\cap C) \\bigr) P(L) P(C) + P \\bigl(X | (L \\cap \\overline{C}) \\bigr) P(L) P(\\overline{C}) + P \\bigl(X | (\\overline{L} \\cap C) \\bigr) P(\\overline{L}) P(C) + P \\bigl(X | (\\overline{L} \\cap \\overline{C}) \\bigr) P(\\overline{L}) P(\\overline{C}) $$\n",
    "$$ = 0.7525 \\times 0.02098 \\times 0.02 + 0.505 \\times 0.02098 \\times 0.98 + 0.505 \\times 0.97902 \\times 0.02 + 0.01 \\times 0.97902 \\times 0.98 = 0.030181249 $$\n",
    "Hence, $$ P(C | X) = \\frac{0.51019255 \\times 0.02}{0.030181249} = 0.33808577637 $$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hence, the probability that someone who has a cough has a cold is 0.33808577637"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q3. Derive the MLE for the parameters of a k-sided multinomial distribution."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suppose there are $k$ categories. Let us do $n$ independent trials, such that each trial leads to the success of exactly one of the $k$ categories. Let $p_i$ be the probability of success and $X_i$ be the number of successes of category $i$ for $i = 1, \\ldots, k$.  \n",
    "Then, the likelihood of the k-sided multinomial distribution with parameters n and $p_i$ for $i = 1, \\ldots, k$ is:\n",
    "$$ f(\\textbf{x}; n, \\textbf{p}) = f(x_1, \\ldots, x_k; n, p_1, \\ldots, p_k) = n! \\prod_{i = 1}^k \\frac{p_i^{x_i}}{x_i!} $$\n",
    "where $\\sum_{i = 1}^{k} x_i = n$ and $\\sum_{i = 1}^{k} p_i = 1$. \n",
    "\n",
    "We will write $f(\\textbf{x}; n, \\textbf{p})$ as $f(\\textbf{p})$ for brevity. \n",
    " \n",
    "Now, the log-likelihood will be:\n",
    "$$ LL(\\textbf{p}) = \\log f(\\textbf{p}) = \\log \\left( n! \\prod_{i = 1}^k \\frac{p_i^{x_i}}{x_i!} \\right) $$\n",
    "$$ = \\log n! + \\log \\prod_{i = 1}^k \\frac{p_i^{x_i}}{x_i!} $$\n",
    "$$ = \\log n! + \\sum_{i = 1}^k \\log \\frac{p_i^{x_i}}{x_i!} $$\n",
    "$$ = \\log n! + \\sum_{i = 1}^k x_i \\log p_i - \\sum_{i = 1}^k \\log x_i! $$\n",
    "\n",
    "We need to maximize $LL(\\textbf{p})$ with the constraint $\\sum_{i = 1}^{k} p_i = 1$. \n",
    "\n",
    "Hence, we can define the lagrangian as:\n",
    "$$ l(\\textbf{p}, \\lambda) = LL(\\textbf{p}) + \\lambda \\left( 1 - \\sum_{i = 1}^k p_i \\right) $$\n",
    "where $\\lambda$ is the lagrange multiplier. \n",
    "\n",
    "Now, to find $\\argmax_{\\textbf{p}} l(\\textbf{p}, \\lambda)$, \n",
    "$$ \\frac{\\partial}{\\partial p_i} l(\\textbf{p}, \\lambda) = 0 $$\n",
    "for $i = 1, \\ldots, k$\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial p_i} l(\\textbf{p}, \\lambda) = \\frac{\\partial}{\\partial p_i} LL(\\textbf{p}) + \\frac{\\partial}{\\partial p_i} \\lambda \\left( 1 - \\sum_{i = 1}^k p_i \\right) $$\n",
    "$$ = \\frac{\\partial}{\\partial p_i} \\sum_{i = 1}^k x_i \\log p_i - \\lambda \\frac{\\partial}{\\partial p_i} \\sum_{i = 1}^k p_i $$\n",
    "$$ = \\frac{x_i}{p_i} - \\lambda $$\n",
    "\n",
    "Since, $\\frac{\\partial}{\\partial p_i} l(\\textbf{p}, \\lambda) = 0$, \n",
    "$$ \\frac{x_i}{p_i} - \\lambda = 0 $$\n",
    "$$ \\Rightarrow p_i = \\frac{x_i}{\\lambda} $$\n",
    "\n",
    "Now, since $\\sum_{i = 1}^{k} p_i = 1$, \n",
    "$$ \\sum_{i = 1}^{k} \\frac{x_i}{\\lambda} = 1 $$\n",
    "$$ \\Rightarrow \\lambda = \\sum_{i = 1}^{k} x_i = n $$\n",
    "\n",
    "Hence, \n",
    "$$ p_i = \\frac{x_i}{n} $$\n",
    "\n",
    "In other words, \n",
    "$$ \\textbf{p} = \\left( \\frac{x_1}{n}, \\ldots, \\frac{x_k}{n} \\right) $$\n",
    "is the MLE for the parameters of a k-sided multinomial distribution. "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}