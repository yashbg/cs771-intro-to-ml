{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CS771A Assignment 1\n",
    "\\- Yash Gupta (190997)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. Gradient Descent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import time as t"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# defining the gradient descent function\n",
    "def gradient_descent(gradient, init_, learn_rate, n_iter=50, tol=1e-06):\n",
    "    x = init_\n",
    "    for _ in range(n_iter):\n",
    "        delta = -learn_rate * gradient(x)\n",
    "        if np.all(np.abs(delta) <= tol):\n",
    "            break\n",
    "        x += delta\n",
    "    return round(x * 1000) / 1000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (a)\n",
    "Use this function to find minima for (i) $x^2 + 3x + 4$ and (ii) $x^4 â€“ 3x^2 + 2x$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we'll have to find the gradient of the expressions in (i) and (ii). As these are expressions in one variable, their gradients are simply there derivatives wrt. x. They can be given as follows:  \n",
    "(i) $2x + 3$  \n",
    "(ii) $4x^3 - 6x + 2$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have the gradients, we can find the minima as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (i)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "min_x1 = gradient_descent(gradient=lambda x: 2 * x + 3, init_=0.0, learn_rate=0.1)\n",
    "print('Point of minima:', min_x1)\n",
    "min_y1 = min_x1 ** 2 + 3 * min_x1 + 4\n",
    "print('Minima:', min_y1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Point of minima: -1.5\n",
      "Minima: 1.75\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (ii)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "source": [
    "min_x2 = gradient_descent(gradient=lambda x: 4 * (x ** 3) - 6 * x + 2, init_=0.0, learn_rate=0.1)\n",
    "print('Point of minima:', min_x2)\n",
    "min_y2 = min_x2 ** 4 - 3 * (min_x2 ** 2) + 2 * min_x2\n",
    "print('Minima:', min_y2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Point of minima: -1.366\n",
      "Minima: -4.848076206064\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hence, "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (b)\n",
    "Write a gradient function to calculate gradients for a linear regression $y = ax + b$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The loss function ($L_2$ loss) for a linear regression $y = ax + b$, say $L(a, b)$, will be: \n",
    "$$ L(a, b) = \\sum_{n = 1}^N (y_n - a x_n - b)^2 $$\n",
    "where N is the number of data points.\n",
    "\n",
    "The gradient for this loss function will be:\n",
    "$$ \\left[ \\frac{\\partial L}{\\partial a}, \\frac{\\partial L}{\\partial b} \\right] = \\left[ -2 \\sum_{n = 1}^N x_n (y_n - a x_n - b), -2 \\sum_{n = 1}^N (y_n - a x_n - b) \\right] $$\n",
    "\n",
    "Hence, the gradient function can be written as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def gradient_lr(params):\n",
    "    grad = np.array([0.0, 0.0])\n",
    "    for Xn, yn in zip(X, y): # X is the features and y is the labels both of which will be global variables\n",
    "        grad[0] += -2 * Xn * (yn - params[0] * Xn - params[1])\n",
    "        grad[1] += -2 * (yn - params[0] * Xn - params[1])\n",
    "    return grad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (c)\n",
    "Generate artificial data for this regression according to the following protocol and use gradient descent to find the optimal parameters relating X with y."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "np.random.seed(0)\n",
    "X = 2.5 * np.random.randn(10000) + 1.5 # array of 10000 values with mean = 1.5, stddev = 2.5\n",
    "res = 1.5 * np.random.randn(10000) # generate 10000 residual terms\n",
    "y = 2 + 0.3 * X + res # actual values of y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we need to make a small modification in the gradient_descent() function defined earlier to support multivariate gradient descent. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "# defining the gradient descent function\n",
    "def gradient_descent_multivariate(gradient, init_, learn_rate, n_iter=50, tol=1e-06):\n",
    "    x = init_\n",
    "    for _ in range(n_iter):\n",
    "        delta = -learn_rate * gradient(x)\n",
    "        if np.all(np.abs(delta) <= tol):\n",
    "            break\n",
    "        x += delta\n",
    "    return np.round(x, 3) # Use np.round() instead of round() to round a numpy array"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running gradient descent on this data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "params_init = np.array([0.0, 0.0])\n",
    "tic = t.process_time()\n",
    "params = gradient_descent_multivariate(gradient_lr, init_=params_init, learn_rate=0.00001)\n",
    "toc = t.process_time()\n",
    "time_vanilla_gd = toc - tic\n",
    "print('Time taken:', time_vanilla_gd, 'seconds')\n",
    "params"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time taken: 1.044782304999984 seconds\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.295, 2.022])"
      ]
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (d)\n",
    "Implement minibatch stochastic gradient descent using the code base you have developed so far."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can define the minibatch stochastic gradient descent function by creating a minibatch in every iteration of the gradient descent and passing the minibatch to the gradient function as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# defining the minibatch stochastic gradient descent function\n",
    "def minibatch_sgd(gradient, init_, learn_rate, n_iter=50, tol=1e-06, batch_size=125):\n",
    "    x = init_\n",
    "    for _ in range(n_iter):\n",
    "        idx = np.random.choice(np.arange(len(X)), batch_size, replace=False) # indexes of the minibatch\n",
    "        X_batch = X[idx]\n",
    "        y_batch = y[idx]\n",
    "        delta = -learn_rate * gradient(x, X_batch, y_batch)\n",
    "        if np.all(np.abs(delta) <= tol):\n",
    "            break\n",
    "        x += delta\n",
    "    return np.round(x, 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will also need to define a gradient function which takes the minibatches as inputs as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "def gradient_lr_sgd(params, X_batch, y_batch):\n",
    "    grad = np.array([0.0, 0.0])\n",
    "    for Xn, yn in zip(X_batch, y_batch):\n",
    "        grad[0] += -2 * Xn * (yn - params[0] * Xn - params[1])\n",
    "        grad[1] += -2 * (yn - params[0] * Xn - params[1])\n",
    "    return grad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running minibatch stochastic gradient descent on the data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "params_sgd_init = np.array([0.0, 0.0])\n",
    "tic = t.process_time()\n",
    "params_sgd = minibatch_sgd(gradient_lr_sgd, init_=params_sgd_init, learn_rate=0.00001)\n",
    "toc = t.process_time()\n",
    "time_sgd = toc - tic\n",
    "print('Time taken:', time_sgd, 'seconds')\n",
    "params"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time taken: 0.032973420000018905 seconds\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.295, 2.022])"
      ]
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (e)\n",
    "Does SGD do better or worse in terms of time performance on our data? Is there an optimal minibatch size that works best? Quantify and interpret your findings."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at the times taken by vanilla gradient descent and minibatch SGD."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "print('Time taken by vanilla gradient descent:', time_vanilla_gd)\n",
    "print('Time taken by minibatch SGD:', time_sgd)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time taken by vanilla gradient descent: 1.044782304999984\n",
      "Time taken by minibatch SGD: 0.032973420000018905\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, minibatch SGD does much better in terms of time performance than vanilla gradient descent. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's find the optimal minibatch size that works best."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "source": [
    "params_sgd_init = np.array([0.0, 0.0])\n",
    "min_time = float('inf')\n",
    "opt_batch_size = 0\n",
    "for batch_size in range(1, 1001, 125):\n",
    "    # print('Current batch size:', batch_size)\n",
    "    tic = t.process_time()\n",
    "    params_sgd = minibatch_sgd(gradient_lr_sgd, init_=params_sgd_init, learn_rate=0.00001)\n",
    "    toc = t.process_time()\n",
    "    time_sgd = toc - tic\n",
    "    if time_sgd < min_time:\n",
    "        min_time = time_sgd\n",
    "        opt_batch_size = batch_size\n",
    "print('The optimal minibatch size is', opt_batch_size, 'and it takes', min_time, 'seconds')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The optimal minibatch size is 751 and it takes 0.019864095000002635 seconds\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "# Interpret your findings"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q2. Bayesian network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Bayesian Network](bayesian_network.png \"Bayesian Network\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q2. (i)\n",
    "Calculate the probability that someone has both cold and a fever"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let $C$ represent cold and $F$ represent fever. So, we need to find $P(C \\cap F)$. \n",
    "$$ P(C \\cap F) = P(F | C) P(C) = 0.307 \\times 0.02 = 0.00614 $$\n",
    "Hence, the probability that someone has both cold and a fever is 0.00614"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q2. (ii)\n",
    "Calculate the probability that someone who has a cough has a cold"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let $X$ represent cough, $C$ represent cold, $L$ represent lung disease and $S$ represent smokes. So, we need to find $P(C | X)$. \n",
    "$$ P(C | X) = \\frac{P(X | C) P(C)}{P(X)} $$\n",
    "Now, $$ P(X | C) = P \\bigl( X | (C \\cap L) \\bigr) P(L | C) + P \\bigl( X | (C \\cap \\overline{L}) \\bigr) P(\\overline{L} | C) $$\n",
    "Since C and L are independent, $P(L | C) = P(L)$ and $P(\\overline{L} | C) = P(\\overline{L})$.\n",
    "So, $$ P(X | C) = P \\bigl( X | (C \\cap L) \\bigr) P(L) + P \\bigl( X | (C \\cap \\overline{L}) \\bigr) P(\\overline{L}) $$\n",
    "Now, $$ P(L) = P(L | S) P(S) + P(L | \\overline{S}) P(\\overline{S}) = 0.1009 \\times 0.2 + 0.001 \\times 0.8 = 0.02098 $$\n",
    "and $$ P(\\overline{L}) = 1 - P(L) = 1 - 0.02098 = 0.97902 $$\n",
    "Hence, $$ P(X | C) = 0.7525 \\times 0.02098 + 0.505 \\times 0.97902 = 0.51019255 $$\n",
    "Now, $$ P(X) = P \\bigl(X | (L \\cap C) \\bigr) P(L \\cap C) + P \\bigl(X | (L \\cap \\overline{C}) \\bigr) P(L \\cap \\overline{C}) + P \\bigl(X | (\\overline{L} \\cap C) \\bigr) P(\\overline{L} \\cap C) + P \\bigl(X | (\\overline{L} \\cap \\overline{C}) \\bigr) P(\\overline{L} \\cap \\overline{C}) $$\n",
    "Again, since L and C are independent, $P(L \\cap C) = P(L) P(C)$,  $P(L \\cap \\overline{C}) = P(L) P(\\overline{C})$, $P(\\overline{L} \\cap C) = P(\\overline{L}) P(C)$ and $P(\\overline{L} \\cap \\overline{C}) = P(\\overline{L}) P(\\overline{C})$  \n",
    "Hence, $$ P(X) = P \\bigl(X | (L \\cap C) \\bigr) P(L) P(C) + P \\bigl(X | (L \\cap \\overline{C}) \\bigr) P(L) P(\\overline{C}) + P \\bigl(X | (\\overline{L} \\cap C) \\bigr) P(\\overline{L}) P(C) + P \\bigl(X | (\\overline{L} \\cap \\overline{C}) \\bigr) P(\\overline{L}) P(\\overline{C}) $$\n",
    "$$ = 0.7525 \\times 0.02098 \\times 0.02 + 0.505 \\times 0.02098 \\times 0.98 + 0.505 \\times 0.97902 \\times 0.02 + 0.01 \\times 0.97902 \\times 0.98 = 0.030181249 $$\n",
    "Hence, $$ P(C | X) = \\frac{0.51019255 \\times 0.02}{0.030181249} = 0.33808577637 $$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hence, the probability that someone who has a cough has a cold is 0.33808577637"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q3. Derive the MLE for the parameters of a k-sided multinomial distribution."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suppose there are $k$ categories. Let us do $n$ independent trials, such that each trial leads to the success of exactly one of the $k$ categories. Let $p_i$ be the probability of success and $X_i$ be the number of successes of category $i$ for $i = 1, \\ldots, k$.  \n",
    "Then, the likelihood of the k-sided multinomial distribution with parameters n and $p_i$ for $i = 1, \\ldots, k$ is:\n",
    "$$ f(\\textbf{x}; n, \\textbf{p}) = f(x_1, \\ldots, x_k; n, p_1, \\ldots, p_k) = n! \\prod_{i = 1}^k \\frac{p_i^{x_i}}{x_i!} $$\n",
    "where $\\sum_{i = 1}^{k} x_i = n$ and $\\sum_{i = 1}^{k} p_i = 1$. \n",
    "\n",
    "We will write $f(\\textbf{x}; n, \\textbf{p})$ as $f(\\textbf{p})$ for brevity. \n",
    " \n",
    "Now, the log-likelihood will be:\n",
    "$$ LL(\\textbf{p}) = \\log f(\\textbf{p}) = \\log \\left( n! \\prod_{i = 1}^k \\frac{p_i^{x_i}}{x_i!} \\right) $$\n",
    "$$ = \\log n! + \\log \\prod_{i = 1}^k \\frac{p_i^{x_i}}{x_i!} $$\n",
    "$$ = \\log n! + \\sum_{i = 1}^k \\log \\frac{p_i^{x_i}}{x_i!} $$\n",
    "$$ = \\log n! + \\sum_{i = 1}^k x_i \\log p_i - \\sum_{i = 1}^k \\log x_i! $$\n",
    "\n",
    "We need to maximize $LL(\\textbf{p})$ with the constraint $\\sum_{i = 1}^{k} p_i = 1$. \n",
    "\n",
    "Hence, we can define the lagrangian as:\n",
    "$$ l(\\textbf{p}, \\lambda) = LL(\\textbf{p}) + \\lambda \\left( 1 - \\sum_{i = 1}^k p_i \\right) $$\n",
    "where $\\lambda$ is the lagrange multiplier. \n",
    "\n",
    "Now, to find $\\argmax_{\\textbf{p}} l(\\textbf{p}, \\lambda)$, \n",
    "$$ \\frac{\\partial}{\\partial p_i} l(\\textbf{p}, \\lambda) = 0 $$\n",
    "for $i = 1, \\ldots, k$\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial p_i} l(\\textbf{p}, \\lambda) = \\frac{\\partial}{\\partial p_i} LL(\\textbf{p}) + \\frac{\\partial}{\\partial p_i} \\lambda \\left( 1 - \\sum_{i = 1}^k p_i \\right) $$\n",
    "$$ = \\frac{\\partial}{\\partial p_i} \\sum_{i = 1}^k x_i \\log p_i - \\lambda \\frac{\\partial}{\\partial p_i} \\sum_{i = 1}^k p_i $$\n",
    "$$ = \\frac{x_i}{p_i} - \\lambda $$\n",
    "\n",
    "Since, $\\frac{\\partial}{\\partial p_i} l(\\textbf{p}, \\lambda) = 0$, \n",
    "$$ \\frac{x_i}{p_i} - \\lambda = 0 $$\n",
    "$$ \\Rightarrow p_i = \\frac{x_i}{\\lambda} $$\n",
    "\n",
    "Now, since $\\sum_{i = 1}^{k} p_i = 1$, \n",
    "$$ \\sum_{i = 1}^{k} \\frac{x_i}{\\lambda} = 1 $$\n",
    "$$ \\Rightarrow \\lambda = \\sum_{i = 1}^{k} x_i = n $$\n",
    "\n",
    "Hence, \n",
    "$$ p_i = \\frac{x_i}{n} $$\n",
    "\n",
    "In other words, \n",
    "$$ \\textbf{p} = \\left( \\frac{x_1}{n}, \\ldots, \\frac{x_k}{n} \\right) $$\n",
    "is the MLE for the parameters of a k-sided multinomial distribution. "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}