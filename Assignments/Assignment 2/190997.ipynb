{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CS771A Assignment 1\n",
    "\\- Yash Gupta (190997)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. Gradient Descent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# defining the gradient descent function\n",
    "def gradient_descent(gradient, init_, learn_rate, n_iter=50, tol=1e-06):\n",
    "    x = init_\n",
    "    for _ in range(n_iter):\n",
    "        delta = -learn_rate * gradient(x)\n",
    "        if np.all(np.abs(delta) <= tol):\n",
    "            break\n",
    "        x += delta\n",
    "    return round(x * 1000) / 1000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (a)\n",
    "Use this function to find minima for (i) $x^2 + 3x + 4$ and (ii) $x^4 â€“ 3x^2 + 2x$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we'll have to find the gradient of the expressions in (i) and (ii). As these are expressions in one variable, their gradients are simply there derivatives wrt. x. They can be given as follows: <br>\n",
    "(i) $2x + 3$ <br>\n",
    "(ii) $4x^3 - 6x + 2$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have the gradients, we can find the minima as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (i)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "min_x1 = gradient_descent(gradient=lambda x: 2 * x + 3, init_=0.0, learn_rate=0.2)\n",
    "print('Point of minima:', min_x1)\n",
    "min_y1 = min_x1 ** 2 + 3 * min_x1 + 4\n",
    "print('Minima:', min_y1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Point of minima: -1.5\n",
      "Minima: 1.75\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (ii)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "min_x2 = gradient_descent(gradient=lambda x: 4 * (x ** 3) - 6 * x + 2, init_=0.0, learn_rate=0.2)\n",
    "print('Point of minima:', min_x2)\n",
    "min_y2 = min_x2 ** 4 - 3 * (min_x2 ** 2) + 2 * min_x2\n",
    "print('Minima:', min_y2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Point of minima: 0.084\n",
      "Minima: 0.146881787136\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hence, "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (b)\n",
    "Write a gradient function to calculate gradients for a linear regression $y = ax + b$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The loss function ($L_2$ loss) for a linear regression $y = ax + b$, say $L(a, b)$, will be: \n",
    "$$ L(a, b) = \\sum_{n = 1}^N (y_n - a x_n - b)^2 $$\n",
    "where N is the number of data points.\n",
    "\n",
    "The gradient for this loss function will be:\n",
    "$$ \\left[ \\frac{\\partial L}{\\partial a}, \\frac{\\partial L}{\\partial b} \\right] = \\left[ -2 \\sum_{n = 1}^N x_n (y_n - a x_n - b), -2 \\sum_{n = 1}^N (y_n - a x_n - b) \\right] $$\n",
    "\n",
    "Hence, the gradient function can be written as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def gradient_lr(params):\n",
    "    grad = np.array([0, 0])\n",
    "    for Xn, yn, in zip(X, y): # X is the features and y is the labels both of which will be global variables\n",
    "        grad[0] += -2 * Xn * (yn - params[0] * Xn - params[1])\n",
    "        grad[1] += -2 * (yn - params[0] * Xn - params[1])\n",
    "    return grad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (c)\n",
    "Generate artificial data for this regression according to the following protocol and use gradient descent to find the optimal parameters relating X with y."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "np.random.seed(0)\n",
    "X = 2.5 * np.random.randn(10000) + 1.5 # array of 100 values with mean = 1.5, stddev = 2.5\n",
    "res = 1.5 * np.random.randn(10000) # generate 100 residual terms\n",
    "y = 2 + 0.3 * X + res # actual values of y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running gradient descent on this data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "params_init = np.array([0, 0])\n",
    "params = gradient_descent(gradient_lr, init_=params_init, learn_rate=0.2)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "UFuncTypeError",
     "evalue": "Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_823868/2384373987.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparams_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_823868/2401161293.py\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(gradient, init_, learn_rate, n_iter, tol)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (d)\n",
    "Implement minibatch stochastic gradient descent using the code base you have developed so far."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def minibatch_sgd(gradient, init_, learn_rate, n_iter=50, tol=1e-06, batch_size=125):\n",
    "    x = init_\n",
    "    n = x.shape[0]\n",
    "    random.shuffle(x)\n",
    "    for _ in range(n_iter):\n",
    "        for start in range(0, n, batch_size):\n",
    "            stop = start + batch_size\n",
    "            x_batch = x[start:stop, :-1]\n",
    "            delta = -learn_rate * gradient(x_batch)\n",
    "            if np.all(np.abs(delta) <= tol):\n",
    "                break\n",
    "            x += delta\n",
    "    return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (e)\n",
    "Does SGD do better or worse in terms of time performance on our data? Is there an optimal minibatch size that works best? Quantify and interpret your findings."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q2. Bayesian network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Bayesian Network](bayesian_network.png \"Bayesian Network\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q2. (i)\n",
    "Calculate the probability that someone has both cold and a fever"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let $C$ represent cold and $F$ represent fever. So, we need to find $P(C \\cap F)$. \n",
    "$$ P(C \\cap F) = P(F | C) P(C) = 0.307 \\times 0.02 = 0.00614 $$\n",
    "Hence, the probability that someone has both cold and a fever is 0.00614"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q2. (ii)\n",
    "Calculate the probability that someone who has a cough has a cold"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let $X$ represent cough, $C$ represent cold and $L$ represent lung disease. So, we need to find $P(C | X)$. \n",
    "$$ P(C | X) = \\frac{P(X | C) P(C)}{P(X)} $$\n",
    "Now, $$ P(X | C) = P \\bigl( X | (C \\cap L) \\bigr) + P \\bigl( X | (C \\cap \\overline{L}) \\bigr) = 0.7525 + 0.505 = 1.2575 $$\n",
    "and $$ P(X) = P \\bigl(X | (L \\cap C) \\bigr) + P \\bigl(X | (L \\cap \\overline{C}) \\bigr) + P \\bigl(X | (\\overline{L} \\cap C) \\bigr) + P \\bigl(X | (\\overline{L} \\cap \\overline{C}) \\bigr) = 0.7525 + 0.505 + 0.505 + 0.01 = 1.7725 $$\n",
    "Hence, $$ P(C | X) = \\frac{1.2575 \\times 0.02}{1.7725} = 0.01418899858 $$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q3. Derive the MLE for the parameters of a k-sided multinomial distribution."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}