{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CS771A Assignment 1\n",
    "\\- Yash Gupta (190997)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. Gradient Descent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import time as t"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# defining the gradient descent function\n",
    "def gradient_descent(gradient, init_, learn_rate, n_iter=50, tol=1e-06):\n",
    "    x = init_\n",
    "    for _ in range(n_iter):\n",
    "        delta = -learn_rate * gradient(x)\n",
    "        if np.all(np.abs(delta) <= tol):\n",
    "            break\n",
    "        x += delta\n",
    "    return round(x * 1000) / 1000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (a)\n",
    "Use this function to find minima for (i) $x^2 + 3x + 4$ and (ii) $x^4 â€“ 3x^2 + 2x$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we'll have to find the gradient of the expressions in (i) and (ii). As these are expressions in one variable, their gradients are simply there derivatives wrt. x. They can be given as follows: <br>\n",
    "(i) $2x + 3$ <br>\n",
    "(ii) $4x^3 - 6x + 2$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have the gradients, we can find the minima as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (i)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "min_x1 = gradient_descent(gradient=lambda x: 2 * x + 3, init_=0.0, learn_rate=0.2)\n",
    "print('Point of minima:', min_x1)\n",
    "min_y1 = min_x1 ** 2 + 3 * min_x1 + 4\n",
    "print('Minima:', min_y1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Point of minima: -1.5\n",
      "Minima: 1.75\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (ii)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "min_x2 = gradient_descent(gradient=lambda x: 4 * (x ** 3) - 6 * x + 2, init_=0.0, learn_rate=0.2)\n",
    "print('Point of minima:', min_x2)\n",
    "min_y2 = min_x2 ** 4 - 3 * (min_x2 ** 2) + 2 * min_x2\n",
    "print('Minima:', min_y2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Point of minima: 0.084\n",
      "Minima: 0.146881787136\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hence, "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (b)\n",
    "Write a gradient function to calculate gradients for a linear regression $y = ax + b$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The loss function ($L_2$ loss) for a linear regression $y = ax + b$, say $L(a, b)$, will be: \n",
    "$$ L(a, b) = \\sum_{n = 1}^N (y_n - a x_n - b)^2 $$\n",
    "where N is the number of data points.\n",
    "\n",
    "The gradient for this loss function will be:\n",
    "$$ \\left[ \\frac{\\partial L}{\\partial a}, \\frac{\\partial L}{\\partial b} \\right] = \\left[ -2 \\sum_{n = 1}^N x_n (y_n - a x_n - b), -2 \\sum_{n = 1}^N (y_n - a x_n - b) \\right] $$\n",
    "\n",
    "Hence, the gradient function can be written as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def gradient_lr(params):\n",
    "    grad = np.array([0.0, 0.0])\n",
    "    for Xn, yn in zip(X, y): # X is the features and y is the labels both of which will be global variables\n",
    "        grad[0] += -2 * Xn * (yn - params[0] * Xn - params[1])\n",
    "        grad[1] += -2 * (yn - params[0] * Xn - params[1])\n",
    "    return grad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (c)\n",
    "Generate artificial data for this regression according to the following protocol and use gradient descent to find the optimal parameters relating X with y."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "np.random.seed(0)\n",
    "X = 2.5 * np.random.randn(10000) + 1.5 # array of 10000 values with mean = 1.5, stddev = 2.5\n",
    "res = 1.5 * np.random.randn(10000) # generate 10000 residual terms\n",
    "y = 2 + 0.3 * X + res # actual values of y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we need to make a small modification in the gradient_descent() function defined earlier to support multivariate gradient descent. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "# defining the gradient descent function\n",
    "def gradient_descent_multivariate(gradient, init_, learn_rate, n_iter=50, tol=1e-06):\n",
    "    x = init_\n",
    "    for _ in range(n_iter):\n",
    "        delta = -learn_rate * gradient(x)\n",
    "        if np.all(np.abs(delta) <= tol):\n",
    "            break\n",
    "        x += delta\n",
    "    return np.round(x, 3) # Use np.round() instead of round() to round a numpy array"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running gradient descent on this data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "params_init = np.array([0.0, 0.0])\n",
    "tic = t.process_time()\n",
    "params = gradient_descent_multivariate(gradient_lr, init_=params_init, learn_rate=0.2)\n",
    "toc = t.process_time()\n",
    "time_vanilla_gd = toc - tic\n",
    "print('Time taken:', time_vanilla_gd, 'seconds')\n",
    "params"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time taken: 3.0262451289999888 seconds\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-2.36058560e+226, -4.58133609e+225])"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (d)\n",
    "Implement minibatch stochastic gradient descent using the code base you have developed so far."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can define the minibatch stochastic gradient descent function by creating a minibatch in every iteration of the gradient descent and passing the minibatch to the gradient function as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# defining the minibatch stochastic gradient descent function\n",
    "def minibatch_sgd(gradient, init_, learn_rate, n_iter=50, tol=1e-06, batch_size=125):\n",
    "    x = init_\n",
    "    for _ in range(n_iter):\n",
    "        idx = np.random.choice(np.arange(len(X)), batch_size, replace=False) # indexes of the minibatch\n",
    "        X_batch = X[idx]\n",
    "        y_batch = y[idx]\n",
    "        delta = -learn_rate * gradient(x, X_batch, y_batch)\n",
    "        if np.all(np.abs(delta) <= tol):\n",
    "            break\n",
    "        x += delta\n",
    "    return np.round(x, 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will also need to define a gradient function which takes the minibatches as inputs as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "def gradient_lr_sgd(params, X_batch, y_batch):\n",
    "    grad = np.array([0.0, 0.0])\n",
    "    for Xn, yn in zip(X_batch, y_batch):\n",
    "        grad[0] += -2 * Xn * (yn - params[0] * Xn - params[1])\n",
    "        grad[1] += -2 * (yn - params[0] * Xn - params[1])\n",
    "    return grad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running minibatch stochastic gradient descent on the data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "params_sgd_init = np.array([0.0, 0.0])\n",
    "tic = t.process_time()\n",
    "params_sgd = minibatch_sgd(gradient_lr_sgd, init_=params_sgd_init, learn_rate=0.2)\n",
    "toc = t.process_time()\n",
    "time_sgd = toc - tic\n",
    "print('Time taken:', time_sgd, 'seconds')\n",
    "params"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time taken: 0.08994101300000068 seconds\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-2.36058560e+226, -4.58133609e+225])"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q1. (e)\n",
    "Does SGD do better or worse in terms of time performance on our data? Is there an optimal minibatch size that works best? Quantify and interpret your findings."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at the times taken by vanilla gradient descent and minibatch SGD."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "print('Time taken by vanilla gradient descent:', time_vanilla_gd)\n",
    "print('Time taken by minibatch SGD:', time_sgd)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time taken by vanilla gradient descent: 3.1711043009999997\n",
      "Time taken by minibatch SGD: 0.08994101300000068\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, minibatch SGD does much better in terms of time performance than vanilla gradient descent. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's find the optimal minibatch size that works best."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "params_sgd_init = np.array([0.0, 0.0])\n",
    "min_time = float('inf')\n",
    "opt_batch_size = 0\n",
    "for batch_size in range(1, 1001, 125):\n",
    "    # print('Current batch size:', batch_size)\n",
    "    tic = t.process_time()\n",
    "    params_sgd = minibatch_sgd(gradient_lr_sgd, init_=params_sgd_init, learn_rate=0.2)\n",
    "    toc = t.process_time()\n",
    "    time_sgd = toc - tic\n",
    "    if time_sgd < min_time:\n",
    "        min_time = time_sgd\n",
    "        opt_batch_size = batch_size\n",
    "print('The optimal minibatch size is', opt_batch_size, 'and it takes', min_time, 'seconds')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_823868/778860591.py:4: RuntimeWarning: overflow encountered in double_scalars\n",
      "  grad[0] += -2 * Xn * (yn - params[0] * Xn - params[1])\n",
      "/tmp/ipykernel_823868/778860591.py:5: RuntimeWarning: overflow encountered in double_scalars\n",
      "  grad[1] += -2 * (yn - params[0] * Xn - params[1])\n",
      "/tmp/ipykernel_823868/778860591.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  grad[0] += -2 * Xn * (yn - params[0] * Xn - params[1])\n",
      "/tmp/ipykernel_823868/778860591.py:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  grad[1] += -2 * (yn - params[0] * Xn - params[1])\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The optimal minibatch size is 251 and it takes 0.06029898100000253 seconds\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "# Interpret your findings"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q2. Bayesian network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Bayesian Network](bayesian_network.png \"Bayesian Network\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q2. (i)\n",
    "Calculate the probability that someone has both cold and a fever"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let $C$ represent cold and $F$ represent fever. So, we need to find $P(C \\cap F)$. \n",
    "$$ P(C \\cap F) = P(F | C) P(C) = 0.307 \\times 0.02 = 0.00614 $$\n",
    "Hence, the probability that someone has both cold and a fever is 0.00614"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q2. (ii)\n",
    "Calculate the probability that someone who has a cough has a cold"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let $X$ represent cough, $C$ represent cold, $L$ represent lung disease and $S$ represent smokes. So, we need to find $P(C | X)$. \n",
    "$$ P(C | X) = \\frac{P(X | C) P(C)}{P(X)} $$\n",
    "Now, $$ P(X | C) = P \\bigl( X | (C \\cap L) \\bigr) P(L | C) + P \\bigl( X | (C \\cap \\overline{L}) \\bigr) P(\\overline{L} | C) $$\n",
    "Since C and L are independent, $P(L | C) = P(L)$ and $P(\\overline{L} | C) = P(\\overline{L})$.\n",
    "So, $$ P(X | C) = P \\bigl( X | (C \\cap L) \\bigr) P(L) + P \\bigl( X | (C \\cap \\overline{L}) \\bigr) P(\\overline{L}) $$\n",
    "Now, $$ P(L) = P(L | S) P(S) + P(L | \\overline{S}) P(\\overline{S}) = 0.1009 \\times 0.2 + 0.001 \\times 0.8 = 0.02098 $$\n",
    "and $$ P(\\overline{L}) = 1 - P(L) = 1 - 0.02098 = 0.97902 $$\n",
    "Hence, $$ P(X | C) = 0.7525 \\times 0.02098 + 0.505 \\times 0.97902 = 0.51019255 $$\n",
    "Now, $$ P(X) = P \\bigl(X | (L \\cap C) \\bigr) P(L \\cap C) + P \\bigl(X | (L \\cap \\overline{C}) \\bigr) P(L \\cap \\overline{C}) + P \\bigl(X | (\\overline{L} \\cap C) \\bigr) P(\\overline{L} \\cap C) + P \\bigl(X | (\\overline{L} \\cap \\overline{C}) \\bigr) P(\\overline{L} \\cap \\overline{C}) $$\n",
    "Again, since L and C are independent, $P(L \\cap C) = P(L) P(C)$,  $P(L \\cap \\overline{C}) = P(L) P(\\overline{C})$, $P(\\overline{L} \\cap C) = P(\\overline{L}) P(C)$ and $P(\\overline{L} \\cap \\overline{C}) = P(\\overline{L}) P(\\overline{C})$ <br>\n",
    "Hence, $$ P(X) = P \\bigl(X | (L \\cap C) \\bigr) P(L) P(C) + P \\bigl(X | (L \\cap \\overline{C}) \\bigr) P(L) P(\\overline{C}) + P \\bigl(X | (\\overline{L} \\cap C) \\bigr) P(\\overline{L}) P(C) + P \\bigl(X | (\\overline{L} \\cap \\overline{C}) \\bigr) P(\\overline{L}) P(\\overline{C}) $$\n",
    "$$ = 0.7525 \\times 0.02098 \\times 0.02 + 0.505 \\times 0.02098 \\times 0.98 + 0.505 \\times 0.97902 \\times 0.02 + 0.01 \\times 0.97902 \\times 0.98 = 0.030181249 $$\n",
    "Hence, $$ P(C | X) = \\frac{0.51019255 \\times 0.02}{0.030181249} = 0.33808577637 $$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hence, the probability that someone who has a cough has a cold is 0.33808577637"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q3. Derive the MLE for the parameters of a k-sided multinomial distribution."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}